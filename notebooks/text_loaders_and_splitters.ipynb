{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Text Loaders**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## UnstructuredURLLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UnstructuredURLLoader of Langchain internally uses unstructured python library to load the content from url's\n",
    "\n",
    "https://unstructured-io.github.io/unstructured/introduction.html\n",
    "\n",
    "https://pypi.org/project/unstructured/#description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import UnstructuredURLLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = UnstructuredURLLoader(\n",
    "    urls=[\n",
    "        \"https://medium.com/@valentinradovich/flood-forecasting-using-deep-learning-and-time-series-f7a27ac4ea2f\",\n",
    "        \"https://medium.com/@valentinradovich/sentiment-analysis-using-hugging-face-transformers-ac35ef60be6a\"\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = loader.load()\n",
    "len(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Flood Forecasting using Deep Learning and Time Series\\n\\nValentin Fernandez Radovich·Follow\\n\\n5 min read·Oct 11\\n\\n--\\n\\nListen\\n\\nShare\\n\\nThis was a ten-day project that I carried out in a group of three persons for Le Wagon last week Data Science Bootcamp.\\n\\nAfter deep investigations we realized a deep learning RNN model that forecast floods using given points of two rivers in the Metropolitan Region of Santiago, Chile. To be more specific we choose to use the LSTM architecture in order to treat the Time Series.\\n\\nThe model tells us the river discharge or flow values for the next 24 hours, indicating whether it is a level that should be considered a warning level or a flooding level. All this was visualized with a Matplotlib in order to be more intuitive for everyone.\\n\\nBut how we did all that?… Okay, first of all, what do you need if you want to create or train a huge deep learning model? Data.\\n\\nThe best way to obtain great data based on the time we had and getting quality information was this API that we found. And what data have we used to train the model? Almost 40 years of historical information!\\n\\nAfter carefully investigating which are the most significant parameters that could or usually make an inference on river flow, we decided to use the following data from the API: temperature, precipitation, surface pressure, shortwave radiation, wind speed, wind direction, soil moisture, and of course, river flow.\\n\\nBefore getting hands-on model training, we had to do some Feature Engineering in order to convert non-cyclical features into cyclical features. In this way, we have relationships between features that are related to each other in real life but not in the data.\\n\\nA real example for this case are the months; with a simple investigation we figured out that the most related months in the history of floods in this particular region in Chile are June, July and August.\\n\\nOn the other hand we have the features of the wind. Specifically wind speed and direction. And, what about these two features? Wind direction is measured in degrees and that means we are going to have artificial discontinuities, a problem that arises when the features have real numbers in wind direction.\\n\\nFor example: we have two data entries, the first is 359 degrees and the second is 1 degree. To the machine it looks like a drastic change, but in reality it is a very small turn.\\n\\nSo, the transformation for cyclican variables avoids this kind of discontinuities. I leave here two images to make it more visualizable for everybody to see how the wind speed and wind direction was related before Feature Engineering and how they are after.\\n\\nLast but not least before model training, we did a typical Normalization. If you are new in this field and need more information about why data should be normalized before training a NN, click here to see a really interesting post I found when I was learning and asking myself why.\\n\\nSo, let’s move forward to Model time. We used an LSTM (Long short-term memory) as an alternative to deal with the vanishing gradient problem who’s present in traditional RNNs.\\n\\nWhy use an LSTM network to forecasts floods?\\n\\nThese types of networks can handle long-term dependencies, which is very important for river time series, since relationships between data over time could extend over several periods.\\n\\nThey are able to capture seasonal and cyclical patterns in the data (just what we are looking for).\\n\\nHave short- and long-term memory, as future values sometimes have dependencies on past events on different time scales.\\n\\nSomething else that everyone needs when modeling for Flood Forecasting is: Use Nash-Sutcliffe Efficiency. This metric (known as NSE) evaluates the ability of a Deep Learning model to predict Time Series. The closer the NSE value is to 1, the better the predictive ability of the model is, and the closer the value is to 0 or less, the worse the performance of the model is.\\n\\nComing to the end, I leave you some images showing the performance of the model to forecast river overflows.\\n\\nIn these image we can see the performance of the model between the years 2015 and 2023 (completely unknown period for the forecast model).\\n\\nIn the three cases of overflows that occurred in the Metropolitan Region in Chile during that period of years, the model had a 100% accuracy performance, ruling those dates as a warning.\\n\\nIn two of the three cases it predicted danger (overflow). And in the remaining case as river levels are in warning.\\n\\nAnd in this last image we visualize at what level of flow the river would have been when the model would have made its forecast (24 hours before) of overflow, avoiding the last catastrophe on August 24 this year.\\n\\nFinally I leave a short video of the demonstration of the web application deployed in Streamlit.\\n\\nAnd this is the end :)\\n\\nIf you are interested in this kind of analysis and explanations feel free to follow me on LinkedIn, GitHub or Medium to see more content like this.\\n\\nThanks everyone for reading ❤.\\n\\nData Science\\n\\nDeep Learning\\n\\nTime Series Forecasting\\n\\nForecasting\\n\\nData\\n\\n--\\n\\n--\\n\\nFollow\\n\\nWritten by Valentin Fernandez Radovich\\n\\n5 Followers\\n\\nData Scientist with background in Software Engineering. Data-driven software solutions enthusiast, a continuous learner and admirer of the latest technologies.\\n\\nFollow\\n\\nMore from Valentin Fernandez Radovich\\n\\nValentin Fernandez Radovich\\n\\nSentiment Analysis using Hugging Face TransformersProject realized for KL Labs.\\n\\n5 min read·Oct 19\\n\\n--\\n\\nValentin Fernandez Radovich\\n\\nSentiment Analysis using Hugging Face TransformersProject realized for KL Labs.\\n\\n5 min read·Oct 19\\n\\n--\\n\\n--\\n\\nSee all from Valentin Fernandez Radovich\\n\\nRecommended from Medium\\n\\nCem ÖZÇELİK\\n\\nin\\n\\nMLearning.ai\\n\\nMultivariate Time Series ForecastingPart I\\xa0: Exploratory Data Analysis & Time Series Analysis\\n\\n27 min read·Jul 2\\n\\n--\\n\\nAnjali Ramesh\\n\\nForecasting Walmart Sales with Machine LearningIn this machine learning project, we utilize historical Walmart sales data to predict store sales. The dataset can be found here.\\n\\n7 min read·Jun 22\\n\\n--\\n\\nLists\\n\\nPredictive Modeling w/ Python20 stories·623 saves\\n\\nNew_Reading_List174 stories·198 saves\\n\\nPractical Guides to Machine Learning10 stories·709 saves\\n\\nNatural Language Processing889 stories·412 saves\\n\\nMarco Peixeiro\\n\\nin\\n\\nTowards Data Science\\n\\nTSMixer: The Latest Forecasting Model by GoogleExplore the architecture of TSMixer and implement it in Python for a long-horizon multivariate forecasting task\\n\\n12 min read·Nov 14\\n\\n--\\n\\ndaython3\\n\\nThe Power of Python: Time-Series Analysis with statsmodels, tslearn, tssearch, and tsfreshPython Time-Series\\n\\n7 min read·Oct 26\\n\\n--\\n\\nBragadeesh Sundararajan\\n\\nBalancing Act: Conquering Imbalanced Data in Classification Like a Pro!In the realm of machine learning and classification tasks, data is the lifeblood that fuels the algorithms, guiding them toward making…\\n\\n12 min read·Sep 16\\n\\n--\\n\\nAmbassador of Newland\\n\\nin\\n\\nThe Data Analytics Academy\\n\\nComparing ARIMA and LSTM for Time Series ForecastingTime series forecasting is a valuable tool for predicting patterns in the flow of time. Two popular models used for time series forecasting…\\n\\n11 min read·Nov 9\\n\\n--\\n\\nSee more recommendations\\n\\nHelp\\n\\nStatus\\n\\nAbout\\n\\nCareers\\n\\nBlog\\n\\nPrivacy\\n\\nTerms\\n\\nText to speech\\n\\nTeams'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].page_content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'https://medium.com/@valentinradovich/flood-forecasting-using-deep-learning-and-time-series-f7a27ac4ea2f'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0].metadata\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Text Splitters**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why do we need text splitters in first place?\n",
    "\n",
    "LLM's have token limits. Hence we need to split the text which can be large into small chunks so that each chunk size is under the token limit. There are various text splitter classes in langchain that allows us to do this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RecursiveTextSplitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nFormal discussions on inference date back to Arab mathematicians and cryptographers, during the Islamic Golden Age between the 8th and 13th centuries. Al-Khalil (717–786) wrote the Book of Cryptographic Messages, which contains one of the first uses of permutations and combinations, to list all possible Arabic words with and without vowels.[15] Al-Kindi\\'s Manuscript on Deciphering Cryptographic Messages gave a detailed description of how to use frequency analysis to decipher encrypted messages, providing an early example of statistical inference for decoding. Ibn Adlan (1187–1268) later made an important contribution on the use of sample size in frequency analysis.[15]\\n\\nAlthough the term \\'statistic\\' was introduced by the Italian scholar Girolamo Ghilini in 1589 with reference to a collection of facts and information about a state, it was the German Gottfried Achenwall in 1749 who started using the term as a collection of quantitative information, in the modern use for this science.[16][17] The earliest writing containing statistics in Europe dates back to 1663, with the publication of Natural and Political Observations upon the Bills of Mortality by John Graunt.[18] Early applications of statistical thinking revolved around the needs of states to base policy on demographic and economic data, hence its stat- etymology. The scope of the discipline of statistics broadened in the early 19th century to include the collection and analysis of data in general. Today, statistics is widely employed in government, business, and natural and social sciences.\\n\\nThe mathematical foundations of statistics developed from discussions concerning games of chance among mathematicians such as Gerolamo Cardano, Blaise Pascal, Pierre de Fermat, and Christiaan Huygens. Although the idea of probability was already examined in ancient and medieval law and philosophy (such as the work of Juan Caramuel), probability theory as a mathematical discipline only took shape at the very end of the 17th century, particularly in Jacob Bernoulli\\'s posthumous work Ars Conjectandi.[19] This was the first book where the realm of games of chance and the realm of the probable (which concerned opinion, evidence, and argument) were combined and submitted to mathematical analysis.[20][21] The method of least squares was first described by Adrien-Marie Legendre in 1805, though Carl Friedrich Gauss presumably made use of it a decade earlier in 1795.[22]\\n\\nThe modern field of statistics emerged in the late 19th and early 20th century in three stages.[23] The first wave, at the turn of the century, was led by the work of Francis Galton and Karl Pearson, who transformed statistics into a rigorous mathematical discipline used for analysis, not just in science, but in industry and politics as well. Galton\\'s contributions included introducing the concepts of standard deviation, correlation, regression analysis and the application of these methods to the study of the variety of human characteristics—height, weight and eyelash length among others.[24] Pearson developed the Pearson product-moment correlation coefficient, defined as a product-moment,[25] the method of moments for the fitting of distributions to samples and the Pearson distribution, among many other things.[26] Galton and Pearson founded Biometrika as the first journal of mathematical statistics and biostatistics (then called biometry), and the latter founded the world\\'s first university statistics department at University College London.[27]\\n\\nThe second wave of the 1910s and 20s was initiated by William Sealy Gosset, and reached its culmination in the insights of Ronald Fisher, who wrote the textbooks that were to define the academic discipline in universities around the world. Fisher\\'s most important publications were his 1918 seminal paper The Correlation between Relatives on the Supposition of Mendelian Inheritance (which was the first to use the statistical term, variance), his classic 1925 work Statistical Methods for Research Workers and his 1935 The Design of Experiments,[28][29][30] where he developed rigorous design of experiments models. He originated the concepts of sufficiency, ancillary statistics, Fisher\\'s linear discriminator and Fisher information.[31] He also coined the term null hypothesis during the Lady tasting tea experiment, which \"is never proved or established, but is possibly disproved, in the course of experimentation\".[32][33] In his 1930 book The Genetical Theory of Natural Selection, he applied statistics to various biological concepts such as Fisher\\'s principle[34] (which A. W. F. Edwards called \"probably the most celebrated argument in evolutionary biology\") and Fisherian runaway,[35][36][37][38][39][40] a concept in sexual selection about a positive feedback runaway effect found in evolution.\\n\\nThe final wave, which mainly saw the refinement and expansion of earlier developments, emerged from the collaborative work between Egon Pearson and Jerzy Neyman in the 1930s. They introduced the concepts of \"Type II\" error, power of a test and confidence intervals. Jerzy Neyman in 1934 showed that stratified random sampling was in general a better method of estimation than purposive (quota) sampling.[41]\\n\\nToday, statistical methods are applied in all fields that involve decision making, for making accurate inferences from a collated body of data and for making decisions in the face of uncertainty based on statistical methodology. The use of modern computers has expedited large-scale statistical computations and has also made possible new methods that are impractical to perform manually. Statistics continues to be an area of active research, for example on the problem of how to analyze big data.[42]\\n'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"\"\"\n",
    "Formal discussions on inference date back to Arab mathematicians and cryptographers, during the Islamic Golden Age between the 8th and 13th centuries. Al-Khalil (717–786) wrote the Book of Cryptographic Messages, which contains one of the first uses of permutations and combinations, to list all possible Arabic words with and without vowels.[15] Al-Kindi's Manuscript on Deciphering Cryptographic Messages gave a detailed description of how to use frequency analysis to decipher encrypted messages, providing an early example of statistical inference for decoding. Ibn Adlan (1187–1268) later made an important contribution on the use of sample size in frequency analysis.[15]\n",
    "\n",
    "Although the term 'statistic' was introduced by the Italian scholar Girolamo Ghilini in 1589 with reference to a collection of facts and information about a state, it was the German Gottfried Achenwall in 1749 who started using the term as a collection of quantitative information, in the modern use for this science.[16][17] The earliest writing containing statistics in Europe dates back to 1663, with the publication of Natural and Political Observations upon the Bills of Mortality by John Graunt.[18] Early applications of statistical thinking revolved around the needs of states to base policy on demographic and economic data, hence its stat- etymology. The scope of the discipline of statistics broadened in the early 19th century to include the collection and analysis of data in general. Today, statistics is widely employed in government, business, and natural and social sciences.\n",
    "\n",
    "The mathematical foundations of statistics developed from discussions concerning games of chance among mathematicians such as Gerolamo Cardano, Blaise Pascal, Pierre de Fermat, and Christiaan Huygens. Although the idea of probability was already examined in ancient and medieval law and philosophy (such as the work of Juan Caramuel), probability theory as a mathematical discipline only took shape at the very end of the 17th century, particularly in Jacob Bernoulli's posthumous work Ars Conjectandi.[19] This was the first book where the realm of games of chance and the realm of the probable (which concerned opinion, evidence, and argument) were combined and submitted to mathematical analysis.[20][21] The method of least squares was first described by Adrien-Marie Legendre in 1805, though Carl Friedrich Gauss presumably made use of it a decade earlier in 1795.[22]\n",
    "\n",
    "The modern field of statistics emerged in the late 19th and early 20th century in three stages.[23] The first wave, at the turn of the century, was led by the work of Francis Galton and Karl Pearson, who transformed statistics into a rigorous mathematical discipline used for analysis, not just in science, but in industry and politics as well. Galton's contributions included introducing the concepts of standard deviation, correlation, regression analysis and the application of these methods to the study of the variety of human characteristics—height, weight and eyelash length among others.[24] Pearson developed the Pearson product-moment correlation coefficient, defined as a product-moment,[25] the method of moments for the fitting of distributions to samples and the Pearson distribution, among many other things.[26] Galton and Pearson founded Biometrika as the first journal of mathematical statistics and biostatistics (then called biometry), and the latter founded the world's first university statistics department at University College London.[27]\n",
    "\n",
    "The second wave of the 1910s and 20s was initiated by William Sealy Gosset, and reached its culmination in the insights of Ronald Fisher, who wrote the textbooks that were to define the academic discipline in universities around the world. Fisher's most important publications were his 1918 seminal paper The Correlation between Relatives on the Supposition of Mendelian Inheritance (which was the first to use the statistical term, variance), his classic 1925 work Statistical Methods for Research Workers and his 1935 The Design of Experiments,[28][29][30] where he developed rigorous design of experiments models. He originated the concepts of sufficiency, ancillary statistics, Fisher's linear discriminator and Fisher information.[31] He also coined the term null hypothesis during the Lady tasting tea experiment, which \"is never proved or established, but is possibly disproved, in the course of experimentation\".[32][33] In his 1930 book The Genetical Theory of Natural Selection, he applied statistics to various biological concepts such as Fisher's principle[34] (which A. W. F. Edwards called \"probably the most celebrated argument in evolutionary biology\") and Fisherian runaway,[35][36][37][38][39][40] a concept in sexual selection about a positive feedback runaway effect found in evolution.\n",
    "\n",
    "The final wave, which mainly saw the refinement and expansion of earlier developments, emerged from the collaborative work between Egon Pearson and Jerzy Neyman in the 1930s. They introduced the concepts of \"Type II\" error, power of a test and confidence intervals. Jerzy Neyman in 1934 showed that stratified random sampling was in general a better method of estimation than purposive (quota) sampling.[41]\n",
    "\n",
    "Today, statistical methods are applied in all fields that involve decision making, for making accurate inferences from a collated body of data and for making decisions in the face of uncertainty based on statistical methodology. The use of modern computers has expedited large-scale statistical computations and has also made possible new methods that are impractical to perform manually. Statistics continues to be an area of active research, for example on the problem of how to analyze big data.[42]\n",
    "\"\"\"\n",
    "text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "r_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators = [\"\\n\\n\", \"\\n\", \" \"],  # List of separators based on requirement (defaults to [\"\\n\\n\", \"\\n\", \" \"])\n",
    "    chunk_size = 200,  # size of each chunk created\n",
    "    chunk_overlap  = 0,  # size of  overlap between chunks in order to maintain the context\n",
    "    length_function = len  # Function to calculate size, currently we are using \"len\" which denotes length of string however you can pass any token counter)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "188\n",
      "194\n",
      "191\n",
      "101\n",
      "191\n",
      "197\n",
      "197\n",
      "197\n",
      "106\n",
      "191\n",
      "195\n",
      "198\n",
      "196\n",
      "89\n",
      "199\n",
      "192\n",
      "194\n",
      "196\n",
      "197\n",
      "80\n",
      "194\n",
      "198\n",
      "199\n",
      "196\n",
      "197\n",
      "193\n",
      "123\n",
      "194\n",
      "198\n",
      "13\n",
      "194\n",
      "193\n",
      "113\n"
     ]
    }
   ],
   "source": [
    "chunks = r_splitter.split_text(text)\n",
    "\n",
    "for chunk in chunks:\n",
    "    print(len(chunk))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This chunks are formed in the following way (Recursivly):\n",
    "\n",
    "#### Iterates through the text and splits, in this case: <br>\n",
    "- ### separators = [\"\\n\\n\", \"\\n\", \" \"]\n",
    "\n",
    "#### And it checks if the text, now chunks, is under the limit, in this case: <br>\n",
    "- ### chunk_size = 200\n",
    "\n",
    "#### If not, it will split the text with the next separator in the list. \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "talklink",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
